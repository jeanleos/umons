{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "notes: \n",
    "\n",
    "- Conv2DTranspose is not available but an equivalent is UpSampling2D + Conv2D\n",
    "- UpSampling2D is also not available so we need to find another model architecture\n",
    "- We can use Resize (keras Resizing) as an equivalent of UpSampling2D\n",
    "- No resize so UpSampling\n",
    "\n",
    "- No conv2D (1, 1)\n",
    "- alveo-u280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "os.environ['PATH'] = os.environ['XILINX_VITIS'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the jet tagging dataset from Open ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_val = np.load('X_train_val.npy')\n",
    "#X_test = np.load('X_test.npy')\n",
    "#y_train_val = np.load('y_train_val.npy')\n",
    "#y_test = np.load('y_test.npy')\n",
    "#classes = np.load('classes.npy', allow_pickle=True)\n",
    "\n",
    "n_samples = 1000\n",
    "X_train_val = np.random.rand(n_samples, size, size, 3).astype(\"float64\")\n",
    "y_train_val = X_train_val[:,:,:,0].copy().astype(\"float32\")  # auto-encoder target is the input itself\n",
    "X_test = np.random.rand(n_samples, size//2, size//2, 1).astype(\"float64\")\n",
    "y_test = X_test[:,:,:,0].copy().astype(\"float32\")  # auto-encoder target is the input itself\n",
    "\n",
    "print(X_train_val.shape, y_train_val.shape)\n",
    "print(X_train_val.dtype, y_train_val.dtype)\n",
    "print(X_train_val[0:5])\n",
    "print(y_train_val[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a model\n",
    "This time we're going to use QKeras layers.\n",
    "QKeras is \"Quantized Keras\" for deep heterogeneous quantization of ML models.\n",
    "\n",
    "https://github.com/google/qkeras\n",
    "\n",
    "It is maintained by Google and we recently added support for QKeras model to hls4ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras import QConv2D, QConv2DBatchnorm\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using `QDense` layer instead of `Dense`, and `QActivation` instead of `Activation`. We're also specifying `kernel_quantizer = quantized_bits(6,0,0)`. This will use 6-bits (of which 0 are integer) for the weights. We also use the same quantization for the biases, and `quantized_relu(6)` for 6-bit ReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_depth_q_autoencoder(input_shape=(32, 32, 3)):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    \n",
    "\n",
    "    model.add(layers.Conv2D(32, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(32, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(64, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(128, kernel_size=(3, 3), padding='same'))\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(1, kernel_size=(3, 3)))\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "\n",
    "    #model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "#model = build_depth_q_autoencoder()\n",
    "#model.summary()\n",
    "\n",
    "from keras.saving import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "model = load_model(\"depth_model64_f.keras\", custom_objects=co, safe_mode=False)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def build_depth_q_autoencoder(input_shape=(size, size, 1), base_filters=8):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(base_filters, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    #x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # dilated convolution stack to capture multi‐scale context\n",
    "    for dilation in [4, 8]:\n",
    "        x = layers.Conv2D(base_filters,\n",
    "            kernel_size=(3, 3), \n",
    "            padding='same', \n",
    "            activation='relu')(x)\n",
    "        #x = layers.BatchNormalization()(x)\n",
    "        #x = layers.UpSampling2D(2)(x)\n",
    "        \n",
    "    for dilation in [4, 8][::-1]:\n",
    "        x = layers.Conv2D(base_filters,\n",
    "            kernel_size=(3, 3), \n",
    "            padding='same', \n",
    "            activation='relu')(x)\n",
    "        #x = layers.BatchNormalization()(x)\n",
    "        #x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "    # final 1×1 projection to single‐channel depth map\n",
    "    outputs = layers.Conv2D(1, (1, 1), padding='same', activation='sigmoid')(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "model = build_depth_q_autoencoder()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def build_depth_q_autoencoder(input_shape=(size, size, 1)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    c1 = layers.Conv2D(8, kernel_size=(3, 3), padding='same', activation='relu')(inputs)\n",
    "    \n",
    "    c1 = layers.Conv2D(8, kernel_size=(3, 3), padding='same', activation='relu')(c1)\n",
    "    \n",
    "    c1 = layers.Conv2D(16, kernel_size=(3, 3), padding='same', activation='relu')(c1)\n",
    "    \n",
    "    c1 = layers.Conv2D(16, kernel_size=(3, 3), padding='same', activation='relu')(c1)\n",
    "    \n",
    "    c1 = layers.Conv2D(8, kernel_size=(3, 3), padding='same', activation='relu')(c1)\n",
    "    \n",
    "    c1 = layers.Conv2D(8, kernel_size=(3, 3), padding='same', activation='relu')(c1)\n",
    "\n",
    "    outputs = layers.Conv2D(1, kernel_size=(3, 3), padding='same', activation='sigmoid')(c1)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = build_depth_q_autoencoder()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "We'll use the same settings as the model for part 1: Adam optimizer with categorical crossentropy loss.\n",
    "The callbacks will decay the learning rate and save the model into a directory 'model_2'\n",
    "The model isn't very complex, so this should just take a few minutes even on the CPU.\n",
    "If you've restarted the notebook kernel after training once, set `train = False` to load the trained model rather than training again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "if train:\n",
    "    #adam = Adam(lr=0.0001)\n",
    "    #model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    #model.compile(optimizer=adam, loss=['binary_crossentropy'], metrics=['accuracy'])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(3e-4),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"],\n",
    "    )\n",
    "    callbacks = all_callbacks(\n",
    "        stop_patience=1000,\n",
    "        lr_factor=0.5,\n",
    "        lr_patience=10,\n",
    "        lr_epsilon=0.000001,\n",
    "        lr_cooldown=2,\n",
    "        lr_minimum=0.0000001,\n",
    "        outputDir='estimate_sign_example',\n",
    "    )\n",
    "    #callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "    model.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        # batch_size=8,\n",
    "        epochs=1,\n",
    "        validation_split=0.25,\n",
    "        # shuffle=True,\n",
    "        # callbacks=callbacks.callbacks,\n",
    "    )\n",
    "    # Save the model again but with the pruning 'stripped' to use the regular layer types\n",
    "    # model = strip_pruning(model)\n",
    "    model.save('estimate_depth_example/KERAS_check_best_model.h5')\n",
    "\"\"\"else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    model = load_model('estimate_depth_example/KERAS_check_best_model.h5', custom_objects=co)\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sw = model.predict(X_test)\n",
    "\n",
    "print(X_test[0:10])\n",
    "print(y_test[0:10])\n",
    "print(y_sw[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB\n",
    "Note as well that the Vitis HLS resource estimates tend to _overestimate_ LUTs, while generally estimating the DSPs correctly. Running the subsequent stages of FPGA compilation reveals the more realistic resource usage, You can run the next step, 'logic synthesis' with `hls_model.build(synth=True, vsynth=True)`, but we skipped it in this tutorial in the interest of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='model')\n",
    "config['Model']['Strategy'] = 'Resource'\n",
    "config['Model']['Precision']['default'] = 'ap_fixed<4,2>'\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config,\n",
    "    output_dir='estimate_depth_example/hls4ml_prj_pynq',\n",
    "    backend='VivadoAccelerator',\n",
    "    board='pynq-z2',\n",
    "    io_type='io_stream'\n",
    ")\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.print_dict(hls4ml.backends.get_backend('VivadoAccelerator').create_initial_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.utils.plot_model(hls_model, show_shapes=True, show_precision=True, to_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(csim=False, export=True, bitfile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n '30,45p' estimate_depth_example/hls4ml_prj_pynq/myproject_vivado_accelerator/project_1.runs/impl_1/design_1_wrapper_utilization_placed.rpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p estimate_depth_example/hls4ml_prj_pynq/package\n",
    "!cp estimate_depth_example/hls4ml_prj_pynq/myproject_vivado_accelerator/project_1.runs/impl_1/design_1_wrapper.bit estimate_depth_example/hls4ml_prj_pynq/package/hls4ml_nn.bit\n",
    "!cp estimate_depth_example/hls4ml_prj_pynq/myproject_vivado_accelerator/project_1.srcs/sources_1/bd/design_1/hw_handoff/design_1.hwh estimate_depth_example/hls4ml_prj_pynq/package/hls4ml_nn.hwh\n",
    "!cp estimate_depth_example/hls4ml_prj_pynq/axi_stream_driver.py estimate_depth_example/hls4ml_prj_pynq/package/\n",
    "#np.save('estimate_depth_example/hls4ml_prj_pynq/package/X_test.npy', X_test)\n",
    "#np.save('estimate_depth_example/hls4ml_prj_pynq/package/y_test.npy', y_test)\n",
    "#!cp X_test.npy y_test.npy estimate_sign_example/hls4ml_prj_pynq/package\n",
    "!cp part7b_deployment.ipynb estimate_depth_example/hls4ml_prj_pynq/package\n",
    "\n",
    "!tar -czvf estimate_depth_example/hls4ml_prj_pynq/package.tar.gz -C estimate_depth_example/hls4ml_prj_pynq/package/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "\n",
    "model = mymodel()\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='model')\n",
    "config['Model']['Strategy'] = 'Resource'\n",
    "\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, \n",
    "    hls_config=config,\n",
    "    output_dir='estimate_depth_example/hls4ml_prj_pynq',\n",
    "    backend='VivadoAccelerator',\n",
    "    board='pynq-z2',\n",
    "    io_type='io_stream'\n",
    ")\n",
    "hls_model.compile()\n",
    "\n",
    "hls_model.build(csim=False, export=True, bitfile=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
